{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 6,
=======
   "execution_count": 1,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%config Completer.use_jedi = False\n",
<<<<<<< Updated upstream
    "# stanza.download('ru')\n",
=======
    "\n",
>>>>>>> Stashed changes
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "import json\n",
    "import pymorphy2\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": 2,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 8,
=======
   "execution_count": 3,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words():\n",
    "    stopwords = []\n",
    "    path_to_file = \"stopwords/Stopwords.txt\"\n",
    "    with open(path_to_file, \"r\", encoding=\"utf-8\") as fl:\n",
    "        for line in fl:\n",
    "            stopwords.append(line.strip(\"\\n\"))\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 9,
=======
   "execution_count": 4,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_form(morph, word):\n",
    "    return morph.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessors"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 10,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
=======
   "execution_count": 6,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "2021-05-28 22:32:35 INFO: Loading these models for language: ru (Russian):\n",
=======
      "2021-05-27 22:47:34 INFO: Loading these models for language: ru (Russian):\n",
>>>>>>> Stashed changes
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
<<<<<<< Updated upstream
      "2021-05-28 22:32:35 INFO: Use device: cpu\n",
      "2021-05-28 22:32:35 INFO: Loading: tokenize\n",
      "2021-05-28 22:32:35 INFO: Loading: pos\n",
      "2021-05-28 22:32:35 INFO: Loading: lemma\n",
      "2021-05-28 22:32:35 INFO: Loading: depparse\n",
      "2021-05-28 22:32:36 INFO: Loading: ner\n",
      "2021-05-28 22:32:37 INFO: Done loading processors!\n"
=======
      "2021-05-27 22:47:34 INFO: Use device: cpu\n",
      "2021-05-27 22:47:34 INFO: Loading: tokenize\n",
      "2021-05-27 22:47:34 INFO: Loading: pos\n",
      "2021-05-27 22:47:34 INFO: Loading: lemma\n",
      "2021-05-27 22:47:34 INFO: Loading: depparse\n",
      "2021-05-27 22:47:35 INFO: Loading: ner\n",
      "2021-05-27 22:47:36 INFO: Done loading processors!\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma,ner,depparse', use_gpu = True)"
=======
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma,ner,depparse', stanza = True)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 14,
=======
   "execution_count": 7,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = load_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data was loaded from source:   https://gorod.mos.ru/"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('temp.csv', sep=\"$\")\n",
    "df = pd.read_excel('Данные по голосованиям.xlsx')"
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('temp.csv', sep=\"$\")"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing & Filtering data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lens\"] = df[\"note\"].apply(lambda x: len(str(x)))"
=======
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lens\"] = df[\"message\"].apply(lambda x: len(str(x)))"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 18,
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df[\"lens\"] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "(8301, 12)"
      ]
     },
     "execution_count": 18,
=======
       "(56698, 3)"
      ]
     },
     "execution_count": 12,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df[\"lens\"] > 50]"
=======
    "df_filtered.shape"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 20,
=======
   "execution_count": 13,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "(6437, 12)"
      ]
     },
     "execution_count": 20,
=======
       "2    24021\n",
       "0    16878\n",
       "1    15799\n",
       "Name: theme_value, dtype: int64"
      ]
     },
     "execution_count": 13,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> Stashed changes
    "df_filtered[\"theme_value\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>theme_value</th>\n",
       "      <th>lens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>По адресу Снежная д24 расположена музыкальная ...</td>\n",
       "      <td>0</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>После проведения работ на кабельной канализаци...</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Очистите опору освещения. Приведите в надлежащ...</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Более двух недель лежит куча грунта в перемешк...</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Осколки бордюрного камня спрятаны за дерево, в...</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  theme_value  lens\n",
       "1  По адресу Снежная д24 расположена музыкальная ...            0   358\n",
       "2  После проведения работ на кабельной канализаци...            0   127\n",
       "3  Очистите опору освещения. Приведите в надлежащ...            0    59\n",
       "4  Более двух недель лежит куча грунта в перемешк...            1    58\n",
       "5  Осколки бордюрного камня спрятаны за дерево, в...            1    68"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df_filtered[df_filtered[\"theme_value\"]==0]\n",
    "df_1 = df_filtered[df_filtered[\"theme_value\"]==1]\n",
    "df_2 = df_filtered[df_filtered[\"theme_value\"]==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = df_0[\"message\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sentences = [sent for corp in full_corpus for sent in sent_tokenize(corp, language=\"russian\")]\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    sentences = [sent for corp in full_corpus for sent in sent_tokenize(corp, language=\"russian\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sents = [i for i in sentences if len(i) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_sents), len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "triplets = []\n",
    "for s in tqdm(long_sents):\n",
    "    doc = nlp(s)\n",
    "    for sent in doc.sentences:\n",
    "            entities = [ent.text for ent in sent.ents]\n",
    "            res_d = dict()\n",
    "            temp_d = dict()\n",
    "            for word in sent.words:\n",
    "                temp_d[word.text] = {\"head\": sent.words[word.head-1].text, \"dep\": word.deprel, \"id\": word.id}\n",
    "            for k in temp_d.keys():\n",
    "                nmod_1 = \"\"\n",
    "                nmod_2 = \"\"\n",
    "                if (temp_d[k][\"dep\"] in [\"nsubj\", \"nsubj:pass\"]) & (k in entities):\n",
    "                    res_d[k] = {\"head\": temp_d[k][\"head\"]}\n",
    "                    \n",
    "                    for k_0 in temp_d.keys():\n",
    "                        if (temp_d[k_0][\"dep\"] in [\"obj\", \"obl\"]) &\\\n",
    "                           (temp_d[k_0][\"head\"] == res_d[k][\"head\"]) &\\\n",
    "                            (temp_d[k_0][\"id\"] > temp_d[res_d[k][\"head\"]][\"id\"]):\n",
    "                            res_d[k][\"obj\"] = k_0\n",
    "                            break\n",
    "                    \n",
    "                    for k_1 in temp_d.keys():\n",
    "                        if (temp_d[k_1][\"head\"] == res_d[k][\"head\"]) & (k_1 == \"не\"):\n",
    "                            res_d[k][\"head\"] = \"не \"+res_d[k][\"head\"]\n",
    "                    \n",
    "                    if \"obj\" in res_d[k].keys():\n",
    "                        for k_4 in temp_d.keys():\n",
    "                            if (temp_d[k_4][\"dep\"] ==\"nmod\") &\\\n",
    "                               (temp_d[k_4][\"head\"] == res_d[k][\"obj\"]):\n",
    "                                nmod_1 = k_4\n",
    "                                break\n",
    "                                \n",
    "                        for k_5 in temp_d.keys():\n",
    "                            if (temp_d[k_5][\"dep\"] ==\"nummod\") &\\\n",
    "                               (temp_d[k_5][\"head\"] == nmod_1):\n",
    "                                nmod_2 = k_5\n",
    "                                break\n",
    "                        res_d[k][\"obj\"] = res_d[k][\"obj\"]+\" \"+nmod_2+\" \"+nmod_1\n",
    "\n",
    "            if len(res_d) > 0:\n",
    "                triplets.append([s, res_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_text = lambda x: \"\".join(i if (i.isdigit()) | (i.isalpha()) | (i in [\" \"]) else \" \" for i in x )\n",
    "\n",
    "clear_triplets = dict()\n",
    "for tr in triplets:\n",
    "    for k in tr[1].keys():\n",
    "        if \"obj\" in tr[1][k].keys():\n",
    "            ## clear_text убрать, если не нужна очистка предложений\n",
    "            clear_triplets[clear_text(tr[0])] =  [k, tr[1][k]['head'], tr[1][k]['obj']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_df = []\n",
    "for k in clear_triplets.keys():\n",
    "    for_df.append([k]+clear_triplets[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DF for prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triplets = pd.DataFrame(for_df, columns=[\"full_sent\", \"subject\", \"verb\", \"object\"])\n",
    "df_triplets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triplets[\"subj_n_f\"] = df_triplets[\"subject\"].apply(lambda x: norm_form(morph, x))\n",
    "df_triplets[\"obj_n_f\"] = df_triplets[\"object\"].apply(lambda x: norm_form(morph, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triplets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_triplets[(~df_triplets[\"subj_n_f\"].isin(stopwords)) &\\\n",
    "                          (~df_triplets[\"obj_n_f\"].isin(stopwords))].sort_values(by=\"obj_n_f\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data on chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = list(chunks(df_filtered[\"obj_n_f\"].unique(), 100))\n",
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_num = 0\n",
    "df_for_draw = df_filtered[df_filtered[\"obj_n_f\"].isin(groups[gr_num])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.unique(df_for_draw[[\"subj_n_f\", \"obj_n_f\"]].values.ravel(\"K\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get edges & edges info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d_d = df_for_draw.drop_duplicates(subset=[\"subj_n_f\", \"obj_n_f\", \"verb\"])[[\"subj_n_f\", \"obj_n_f\", \"verb\", \"full_sent\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d_d.shape, df_for_draw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = dict()\n",
    "label_dict = dict()\n",
    "for cc, raw in enumerate(df_d_d.values):\n",
    "    info_dict[(raw[0], raw[1])] = {f\"sent_{cc}\": raw[3]}\n",
    "    label_dict[(raw[0], raw[1])] = raw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_num = dict()\n",
    "for c, word in enumerate(nodes):\n",
    "    word_num[word] = c+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import script_for_graph\n",
    "import importlib\n",
    "importlib.reload(script_for_graph)\n",
    "from script_for_graph import header_text, tail_text\n",
    "\n",
    "header_text += \"\"\"\\nvar nodes = new vis.DataSet([\\n\"\"\"\n",
    "for w in nodes:\n",
    "    header_text += \"{\"\n",
    "    header_text += f\"\"\"         id: {word_num[w]}, \n",
    "                                label: \"{w}\"\\n\"\"\"\n",
    "    header_text += \"},\"\n",
    "header_text += \"   ]);\\n\"\n",
    "\n",
    "header_text += \"\"\"var edges = new vis.DataSet([\"\"\"\n",
    "for k in info_dict.keys():\n",
    "    header_text += \"{\"\n",
    "    header_text += f\"\"\"       from: {word_num[k[0]]}, \n",
    "                    to: {word_num[k[1]]}, \n",
    "                    arrows: \"to\",\n",
    "                    label: \"{label_dict[k]}\",\n",
    "                    info: {info_dict[k]}\\n\"\"\"\n",
    "    header_text +=\"},\"\n",
    "header_text += \"   ]);\\n\"\n",
    "\n",
    "full_text = \"\"\n",
    "full_text += header_text\n",
    "full_text += tail_text\n",
    "\n",
    "with open(f\"Graph_for_group_{gr_num}.html\", \"w\", encoding=\"utf-8\") as f: \n",
    "    f.write(full_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.7.6"
=======
   "version": "3.8.3"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
