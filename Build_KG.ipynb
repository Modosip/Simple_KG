{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 6,
=======
   "execution_count": 1,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%config Completer.use_jedi = False\n",
    "# stanza.download('ru')\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "import json\n",
    "import pymorphy2\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": 2,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 8,
=======
   "execution_count": 3,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words():\n",
    "    stopwords = []\n",
    "    path_to_file = \"stopwords/Stopwords.txt\"\n",
    "    with open(path_to_file, \"r\", encoding=\"utf-8\") as fl:\n",
    "        for line in fl:\n",
    "            stopwords.append(line.strip(\"\\n\"))\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 9,
=======
   "execution_count": 4,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_form(morph, word):\n",
    "    return morph.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessors"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 10,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
=======
   "execution_count": 6,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "2021-05-28 22:32:35 INFO: Loading these models for language: ru (Russian):\n",
=======
      "2021-05-27 22:47:34 INFO: Loading these models for language: ru (Russian):\n",
>>>>>>> Stashed changes
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
<<<<<<< Updated upstream
      "2021-05-28 22:32:35 INFO: Use device: cpu\n",
      "2021-05-28 22:32:35 INFO: Loading: tokenize\n",
      "2021-05-28 22:32:35 INFO: Loading: pos\n",
      "2021-05-28 22:32:35 INFO: Loading: lemma\n",
      "2021-05-28 22:32:35 INFO: Loading: depparse\n",
      "2021-05-28 22:32:36 INFO: Loading: ner\n",
      "2021-05-28 22:32:37 INFO: Done loading processors!\n"
=======
      "2021-05-27 22:47:34 INFO: Use device: cpu\n",
      "2021-05-27 22:47:34 INFO: Loading: tokenize\n",
      "2021-05-27 22:47:34 INFO: Loading: pos\n",
      "2021-05-27 22:47:34 INFO: Loading: lemma\n",
      "2021-05-27 22:47:34 INFO: Loading: depparse\n",
      "2021-05-27 22:47:35 INFO: Loading: ner\n",
      "2021-05-27 22:47:36 INFO: Done loading processors!\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma,ner,depparse', stanza = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 14,
=======
   "execution_count": 7,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = load_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data was loaded from source:   https://gorod.mos.ru/"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 15,
=======
   "execution_count": 9,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('temp.csv', sep=\"$\")\n",
    "df = pd.read_excel('Данные по голосованиям.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing & Filtering data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 17,
=======
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lens\"] = df[\"note\"].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 19,
=======
   "execution_count": 26,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df[\"lens\"] > 50]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 20,
=======
   "execution_count": 27,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "(6437, 12)"
      ]
     },
     "execution_count": 20,
=======
       "(3116, 3)"
      ]
     },
     "execution_count": 27,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 28,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1092\n",
       "1    1026\n",
       "0     998\n",
       "Name: theme_value, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered[\"theme_value\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>theme_value</th>\n",
       "      <th>lens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>По адресу Снежная д24 расположена музыкальная ...</td>\n",
       "      <td>0</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>После проведения работ на кабельной канализаци...</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Очистите опору освещения. Приведите в надлежащ...</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Более двух недель лежит куча грунта в перемешк...</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Осколки бордюрного камня спрятаны за дерево, в...</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  theme_value  lens\n",
       "1  По адресу Снежная д24 расположена музыкальная ...            0   358\n",
       "2  После проведения работ на кабельной канализаци...            0   127\n",
       "3  Очистите опору освещения. Приведите в надлежащ...            0    59\n",
       "4  Более двух недель лежит куча грунта в перемешк...            1    58\n",
       "5  Осколки бордюрного камня спрятаны за дерево, в...            1    68"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 30,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df_filtered[df_filtered[\"theme_value\"]==0]\n",
    "df_1 = df_filtered[df_filtered[\"theme_value\"]==1]\n",
    "df_2 = df_filtered[df_filtered[\"theme_value\"]==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 22,
=======
   "execution_count": 31,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = df_filtered[\"note\"].values"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 30,
=======
   "execution_count": 33,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error Tunnel connection\n",
      "[nltk_data]     failed: 407 Proxy Authentication Required>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error Tunnel connection\n",
      "[nltk_data]     failed: 407 Proxy Authentication Required>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/russian.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\skrochinskiy_ki/nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-18359e3a538c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull_corpus\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"russian\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-18359e3a538c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull_corpus\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"russian\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/russian.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\skrochinskiy_ki/nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-18359e3a538c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'punkt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull_corpus\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"russian\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-18359e3a538c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'punkt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull_corpus\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"russian\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/russian.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\skrochinskiy_ki/nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\skrochinskiy_ki\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sentences = [sent for corp in full_corpus for sent in sent_tokenize(corp, language=\"russian\")]\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    sentences = [sent for corp in full_corpus for sent in sent_tokenize(corp, language=\"russian\")]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 34,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sents = [i for i in sentences if len(i) > 20]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 35,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1922, 2194)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(long_sents), len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Triplets"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1922/1922 [09:31<00:00,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "%%time\n",
    "triplets = []\n",
    "for s in tqdm(long_sents):\n",
    "    doc = nlp(s)\n",
    "    for sent in doc.sentences:\n",
    "            entities = [ent.text for ent in sent.ents]\n",
    "            res_d = dict()\n",
    "            temp_d = dict()\n",
    "            for word in sent.words:\n",
    "                temp_d[word.text] = {\"head\": sent.words[word.head-1].text, \"dep\": word.deprel, \"id\": word.id}\n",
    "            for k in temp_d.keys():\n",
    "                nmod_1 = \"\"\n",
    "                nmod_2 = \"\"\n",
    "                if (temp_d[k][\"dep\"] in [\"nsubj\", \"nsubj:pass\"]) & (k in entities):\n",
    "                    res_d[k] = {\"head\": temp_d[k][\"head\"]}\n",
    "                    \n",
    "                    for k_0 in temp_d.keys():\n",
    "                        if (temp_d[k_0][\"dep\"] in [\"obj\", \"obl\"]) &\\\n",
    "                           (temp_d[k_0][\"head\"] == res_d[k][\"head\"]) &\\\n",
    "                            (temp_d[k_0][\"id\"] > temp_d[res_d[k][\"head\"]][\"id\"]):\n",
    "                            res_d[k][\"obj\"] = k_0\n",
    "                            break\n",
    "                    \n",
    "                    for k_1 in temp_d.keys():\n",
    "                        if (temp_d[k_1][\"head\"] == res_d[k][\"head\"]) & (k_1 == \"не\"):\n",
    "                            res_d[k][\"head\"] = \"не \"+res_d[k][\"head\"]\n",
    "                    \n",
    "                    if \"obj\" in res_d[k].keys():\n",
    "                        for k_4 in temp_d.keys():\n",
    "                            if (temp_d[k_4][\"dep\"] ==\"nmod\") &\\\n",
    "                               (temp_d[k_4][\"head\"] == res_d[k][\"obj\"]):\n",
    "                                nmod_1 = k_4\n",
    "                                break\n",
    "                                \n",
    "                        for k_5 in temp_d.keys():\n",
    "                            if (temp_d[k_5][\"dep\"] ==\"nummod\") &\\\n",
    "                               (temp_d[k_5][\"head\"] == nmod_1):\n",
    "                                nmod_2 = k_5\n",
    "                                break\n",
    "                        res_d[k][\"obj\"] = res_d[k][\"obj\"]+\" \"+nmod_2+\" \"+nmod_1\n",
    "\n",
    "            if len(res_d) > 0:\n",
    "                triplets.append([s, res_d])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 37,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_text = lambda x: \"\".join(i if (i.isdigit()) | (i.isalpha()) | (i in [\" \"]) else \" \" for i in x )\n",
    "\n",
    "clear_triplets = dict()\n",
    "for tr in triplets:\n",
    "    for k in tr[1].keys():\n",
    "        if \"obj\" in tr[1][k].keys():\n",
    "            ## clear_text убрать, если не нужна очистка предложений\n",
    "            clear_triplets[clear_text(tr[0])] =  [k, tr[1][k]['head'], tr[1][k]['obj']] "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 38,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "for_df = []\n",
    "for k in clear_triplets.keys():\n",
    "    for_df.append([k]+clear_triplets[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DF for prepare"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 39,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_triplets = pd.DataFrame(for_df, columns=[\"full_sent\", \"subject\", \"verb\", \"object\"])\n",
    "df_triplets.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 40,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triplets[\"subj_n_f\"] = df_triplets[\"subject\"].apply(lambda x: norm_form(morph, x))\n",
    "df_triplets[\"obj_n_f\"] = df_triplets[\"object\"].apply(lambda x: norm_form(morph, x))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_sent</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "      <th>subj_n_f</th>\n",
       "      <th>obj_n_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>МОЭСК во время проведения аварийно восстановит...</td>\n",
       "      <td>МОЭСК</td>\n",
       "      <td>вскрыло</td>\n",
       "      <td>покрытие</td>\n",
       "      <td>моэск</td>\n",
       "      <td>покрытие</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Электрики всячески оттягивают выполнение работ</td>\n",
       "      <td>Электрики</td>\n",
       "      <td>оттягивают</td>\n",
       "      <td>выполнение  работ</td>\n",
       "      <td>электрик</td>\n",
       "      <td>выполнение  работа</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ямы находятся на траектории движения во двор</td>\n",
       "      <td>Ямы</td>\n",
       "      <td>находятся</td>\n",
       "      <td>траектории  движения</td>\n",
       "      <td>ям</td>\n",
       "      <td>траектории  движение</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Волга очень давно стоит на одном месте  тем са...</td>\n",
       "      <td>Волга</td>\n",
       "      <td>стоит</td>\n",
       "      <td>месте</td>\n",
       "      <td>волга</td>\n",
       "      <td>месте</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Четыре недели назад прокладывая коммуникации в...</td>\n",
       "      <td>Стрелецкая</td>\n",
       "      <td>засыпали</td>\n",
       "      <td>щебней</td>\n",
       "      <td>стрелецкий</td>\n",
       "      <td>щебней</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_sent     subject        verb  \\\n",
       "0  МОЭСК во время проведения аварийно восстановит...       МОЭСК     вскрыло   \n",
       "1    Электрики всячески оттягивают выполнение работ    Электрики  оттягивают   \n",
       "2      Ямы находятся на траектории движения во двор          Ямы   находятся   \n",
       "3  Волга очень давно стоит на одном месте  тем са...       Волга       стоит   \n",
       "4  Четыре недели назад прокладывая коммуникации в...  Стрелецкая    засыпали   \n",
       "\n",
       "                 object    subj_n_f               obj_n_f  \n",
       "0            покрытие         моэск            покрытие    \n",
       "1     выполнение  работ    электрик    выполнение  работа  \n",
       "2  траектории  движения          ям  траектории  движение  \n",
       "3               месте         волга               месте    \n",
       "4              щебней    стрелецкий              щебней    "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "df_triplets.head(5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 42,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_triplets[(~df_triplets[\"subj_n_f\"].isin(stopwords)) &\\\n",
    "                          (~df_triplets[\"obj_n_f\"].isin(stopwords))].sort_values(by=\"obj_n_f\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 43,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 6)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_sent</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "      <th>subj_n_f</th>\n",
       "      <th>obj_n_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Бытовки убрали  информационный щит тоже  а гор...</td>\n",
       "      <td>Бытовки</td>\n",
       "      <td>убрали</td>\n",
       "      <td>щит</td>\n",
       "      <td>бытовка</td>\n",
       "      <td>щит</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Четыре недели назад прокладывая коммуникации в...</td>\n",
       "      <td>Стрелецкая</td>\n",
       "      <td>засыпали</td>\n",
       "      <td>щебней</td>\n",
       "      <td>стрелецкий</td>\n",
       "      <td>щебней</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Дворники признают что территория их но почему ...</td>\n",
       "      <td>Дворники</td>\n",
       "      <td>признают</td>\n",
       "      <td>что</td>\n",
       "      <td>дворник</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_sent     subject      verb  \\\n",
       "0  Бытовки убрали  информационный щит тоже  а гор...     Бытовки    убрали   \n",
       "1  Четыре недели назад прокладывая коммуникации в...  Стрелецкая  засыпали   \n",
       "2  Дворники признают что территория их но почему ...    Дворники  признают   \n",
       "\n",
       "     object    subj_n_f   obj_n_f  \n",
       "0     щит       бытовка     щит    \n",
       "1  щебней    стрелецкий  щебней    \n",
       "2     что       дворник     что    "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "df_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data on chunks"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 45,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = list(chunks(df_filtered[\"obj_n_f\"].unique(), 100))\n",
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 46,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_num = 0\n",
    "df_for_draw = df_filtered[df_filtered[\"obj_n_f\"].isin(groups[gr_num])]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 47,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.unique(df_for_draw[[\"subj_n_f\", \"obj_n_f\"]].values.ravel(\"K\"))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 48,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get edges & edges info"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 49,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d_d = df_for_draw.drop_duplicates(subset=[\"subj_n_f\", \"obj_n_f\", \"verb\"])[[\"subj_n_f\", \"obj_n_f\", \"verb\", \"full_sent\"]]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 50,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 4), (25, 6))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_d_d.shape, df_for_draw.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 51,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = dict()\n",
    "label_dict = dict()\n",
    "for cc, raw in enumerate(df_d_d.values):\n",
    "    info_dict[(raw[0], raw[1])] = {f\"sent_{cc}\": raw[3]}\n",
    "    label_dict[(raw[0], raw[1])] = raw[2]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 52,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "word_num = dict()\n",
    "for c, word in enumerate(nodes):\n",
    "    word_num[word] = c+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Graph"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 53,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import script_for_graph\n",
    "import importlib\n",
    "importlib.reload(script_for_graph)\n",
    "from script_for_graph import header_text, tail_text\n",
    "\n",
    "header_text += \"\"\"\\nvar nodes = new vis.DataSet([\\n\"\"\"\n",
    "for w in nodes:\n",
    "    header_text += \"{\"\n",
    "    header_text += f\"\"\"         id: {word_num[w]}, \n",
    "                                label: \"{w}\"\\n\"\"\"\n",
    "    header_text += \"},\"\n",
    "header_text += \"   ]);\\n\"\n",
    "\n",
    "header_text += \"\"\"var edges = new vis.DataSet([\"\"\"\n",
    "for k in info_dict.keys():\n",
    "    header_text += \"{\"\n",
    "    header_text += f\"\"\"       from: {word_num[k[0]]}, \n",
    "                    to: {word_num[k[1]]}, \n",
    "                    arrows: \"to\",\n",
    "                    label: \"{label_dict[k]}\",\n",
    "                    info: {info_dict[k]}\\n\"\"\"\n",
    "    header_text +=\"},\"\n",
    "header_text += \"   ]);\\n\"\n",
    "\n",
    "full_text = \"\"\n",
    "full_text += header_text\n",
    "full_text += tail_text\n",
    "\n",
    "with open(f\"Graph_for_group_{gr_num}.html\", \"w\", encoding=\"utf-8\") as f: \n",
    "    f.write(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.7.6"
=======
   "version": "3.8.3"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
